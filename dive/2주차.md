-2주차 활동 목표 : 문채운 선배님의 레포 wisper 분석하고 모르는 용어 이해하기


BPL STalk Model Research 

import
1. intel_npu_acceleration_library
2. gc
3. torch
4. torchaudio
5. wespeaker
from~ import~
1. llama_cpp, Llama
2. pyannote.audio, Audio
3. pyannote.core, Segment
4. faster_wisper, WhisperModel
5. huggigface_hub, hf_hub_download

-import inter_npu_acceleration_libary
-import torch
-import torchaudio
버전 2.0.0 이상의 pytorch에서 Torch 컴파일을 사용하여 NPU에 맞게 모델을 최적화하는 외장함수를 불러오는 코드이다.
이때 NPU는 컴퓨터 가속화와 데이터 전송 기능 등이 포함되어있다. 
[https://github.com/intel/intel-npu-acceleration-library]

-import gc
gc는 레퍼런스 카운트를 기준으로 진행한다. (레퍼런스 카운트: 모든 객체가 참조당할 때 증가시키고 참조가 없어지면 감소시키는 것, 값이 0이 되면 객체가 메모리에서 해제됨)
gc에서는 세대 구분을 한다. 객체들은 총 3단계의 세대(g0, g1, g2)로 구분된다.  
- 0세대, 메모리에 객체 할당 횟수에 해제된 횟수를 뺀 값으로, 객체 수를 의미한다. 임계값을 초과하면 0세대 GC를 수행한다.
- 1세대, 0세대 GC 이후 살아남은 객체를 1세대로 이동시킨 후 카운터를 1 증가시키며, 이 1세대 카운터가 임계값을 초과하면 1세대 GC를 수행한다.
- 2세대 GC는 1세대 GC처럼, 1세대 GC에서 살아남은 객체들에 동일한 방식으로 GC를 수행한다.

출처: [https://twinparadox.tistory.com/623](https://twinparadox.tistory.com/623) [Twinparadox Factory:티스토리]

-import wespeaker
음성 식별과 음성 인식을 위한 라이브러리이다.


-라이브러리를 불러온 후
먼저 cuda가 사용 가능한지 확인한다. cuda는 파이썬에서 gpu프로그래밍을 시작하는 방법이다. 계산이 느린 cpu와는 다르게 gpu는 컴퓨터에서 계산을 매우 빠르게 해주기 때문에 딥러닝에서 많이 쓰인다. 
기본적으로 device는 cuda로 설정되어있고, 만약에 cuda를 사용할 수 없으면 device는 cpu로 설정된다. 

-chathistory 
먼저 기본적으로 빈 list를 받는다. 

그리고 remove_last 함수를 받아서 messages 리스트에서 마지막 메시지를 제거한다.

add_messages 함수를 받아서 메시지를 추가한다. 만약에 content가 문자열이면 'role'에 role를, 'content'에 content를 추가한다. 그렇지 않다면 여러 메시지를 추가한다. role과 content를 반복하면서 그 메시지를 추가하는 것이다.


-whisper

get_whisper 함수는 whisper모델의 음성을 텍스트로 변환하는 transcribe 함수를 사용한다. 

먼저 함수를 정의해주고, model_size에서 모델의 크기를 지정한다. (ex. medium) 모델의 크기는 클수록 성능이 향상되지만 더 많은 계산을 필요로 한다.
compute_type는 계산 방식을 지정한다. (ex.int8) int8과 같은 정밀도 방식은 더 적은 메모리를 사용하지만 성능이 떨어질 수 있다. 

이제 WhisperModel을 반환해준다.
model_size와 device (cpu/cuda), cpu_threads, compute_type등을 지정해서 WhisperModel생성 후 transcribe를 호출하여 반환한다.